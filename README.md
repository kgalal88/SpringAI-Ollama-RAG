# SpringAI Ollama RAG

A minimal **Retrieval-Augmented Generation (RAG)** example built with **Spring Boot**, **Spring AI**, and **Ollama**. This project demonstrates how to:

* Run a local LLM using **Ollama** (e.g., Llama, Mistral, Gemma)
* Index documents using embeddings
* Retrieve relevant context from a vector store
* Augment a model prompt with retrieved context
* Provide an API for answering user questions over your data

---

## ðŸš€ Features

* **Spring Boot 3+** application using Spring AI
* **Ollama** as the local LLM backend
* **RAG pipeline** (Embeddings â†’ Vector Store â†’ Retrieval â†’ LLM)
* **REST endpoints** to query your documents
* **Dockerfile** included for containerization
* Lightweight, easy to extend

---

## ðŸ“¦ Project Structure

```
SpringAI-Ollama-RAG/
â”œâ”€â”€ src/main/java/...       # Spring Boot source code
â”œâ”€â”€ src/main/resources/     # Configuration, application.yml
â”œâ”€â”€ pom.xml                  # Maven dependencies
â”œâ”€â”€ Dockerfile               # Container build
â”œâ”€â”€ setup.txt                # Setup notes / instructions
â””â”€â”€ README.md                # Project documentation
```

---

## ðŸ§° Prerequisites

* **Java 17+**
* **Maven 3.9+**
* **Ollama** installed locally â†’ [https://ollama.ai](https://ollama.ai)
* (Optional) **Docker**

---

## ðŸ”§ Setup Instructions

1. **Install Ollama**

```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

2. **Pull an LLM Model** (example: llama3)

```bash
ollama pull llama3
```

3. **Run the Model**

```bash
ollama run llama3
```

4. **Build the Project**

```bash
mvn clean package -DskipTests
```

5. **Run the Spring Boot Application**

```bash
java -jar target/springai-ollama-rag.jar
```

---

## ðŸ“š API Endpoints

### **POST /api/rag/query**

Send a question and receive an answer augmented with indexed context.

Example Request:

```json
{
  "query": "What is in the repository?"
}
```

Example Response:

```json
{
  "answer": "This project contains a Spring Boot RAG example..."
}
```

---

## ðŸ“˜ How RAG Works in This Project

1. **Documents** are loaded from `/resources/data` or another location
2. **Embeddings** are generated using the model via Spring AI
3. Embeddings are stored in a **vector store** (in-memory or persistent)
4. For every user query:

   * Similar documents are retrieved using cosine similarity
   * Retrieved context is injected into the model prompt
   * Ollama generates an answer

---

## ðŸ³ Docker Support

To build the container:

```bash
docker build -t springai-rag .
```

To run:

```bash
docker run -p 8080:8080 springai-rag
```

> Ensure that the container can reach your **local Ollama instance**. You may need to expose Ollama with `OLLAMA_HOST=0.0.0.0`.

---

## ðŸ§ª Testing the RAG Flow

You can test via Postman, curl, or any HTTP tool:

```bash
curl -X POST http://localhost:8080/api/rag/query \
  -H "Content-Type: application/json" \
  -d '{"query": "Explain the RAG architecture."}'
```

---

## ðŸ›  Configuration

Edit `application.yml` to modify:

* Ollama host
* Model name
* Vector store type
* Embedding model
* Logging & performance

---

## ðŸŒ± Future Enhancements

* File upload endpoint for dynamic ingestion
* Support for PostgreSQL/Redis vector stores
* Chunking strategies
* Metadata filtering
* Streaming responses

---

## ðŸ“„ License

This project is open-source under the **Apache-2.0 License**.

---

## ðŸ‘¤ Author

**Khalid Galal**

If you want me to also generate:

* `CONTRIBUTING.md`
* API documentation with examples
* A full architecture diagram

Just tell me!

